import numpy as np
import pandas as pd
from typing import Dict, List

def strategy(df: pd.DataFrame, config_dict: dict) -> Dict[str, float]:
    """
    Minute-frequency, cross-sectional, market-neutral crypto strategy on a fixed universe of 10 symbols using price-only data.

    Returns a dictionary of symbol -> weight, where sum of absolute weights <= 1.0.
    """

    # =========================
    # ------ VALIDATION -------
    # =========================
    if not isinstance(df, pd.DataFrame):
        raise TypeError("Input must be a pandas DataFrame.")
    if not isinstance(df.index, pd.DatetimeIndex):
        raise TypeError("DataFrame index must be a pandas DatetimeIndex.")
    if df.shape[0] < 5:
        raise ValueError("DataFrame must contain multiple rows (minute bars).")
    if config_dict is None or not isinstance(config_dict, dict):
        raise TypeError("config_dict must be a dict.")

    strategy_specific_config = config_dict.get("strategy_config", {})
    # Required params with defaults/fallbacks
    symbols: List[str] = strategy_specific_config.get("symbols", ["BTCUSDT","ETHUSDT","XRPUSDT","BCHUSDT","LTCUSDT","ADAUSDT","ETCUSDT","TRXUSDT","DOTUSDT","DOGEUSDT"])
    leader_btc: str = strategy_specific_config.get("leader_btc", "BTCUSDT")
    leader_eth: str = strategy_specific_config.get("leader_eth", "ETHUSDT")

    group_pow: List[str] = strategy_specific_config.get("group_pow", ["BCHUSDT","ETCUSDT"])
    group_platform: List[str] = strategy_specific_config.get("group_platform", ["ADAUSDT","DOTUSDT"])
    group_meme: List[str] = strategy_specific_config.get("group_meme", ["DOGEUSDT","TRXUSDT"])
    group_other: List[str] = strategy_specific_config.get("group_other", ["BTCUSDT","ETHUSDT","XRPUSDT","LTCUSDT"])

    leadlag_lags: List[int] = strategy_specific_config.get("leadlag_lags", [1, 3])
    leadlag_lookback: int = strategy_specific_config.get("leadlag_lookback", 720)

    momentum_windows: List[int] = strategy_specific_config.get("momentum_windows", [30, 60, 90])
    beta_lookback: int = strategy_specific_config.get("beta_lookback", 720)

    reversal_windows: List[int] = strategy_specific_config.get("reversal_windows", [1, 2, 3])

    vr_q: int = strategy_specific_config.get("vr_q", 30)
    vr_gate_strength: float = strategy_specific_config.get("vr_gate_strength", 1.0)

    roll_cov_window: int = strategy_specific_config.get("roll_cov_window", 60)
    cost_kappa: float = strategy_specific_config.get("cost_kappa", 0.5)

    forecast_horizon: int = strategy_specific_config.get("forecast_horizon", 15)
    ic_halflife: float = float(strategy_specific_config.get("ic_halflife", 600.0))
    softmax_beta: float = strategy_specific_config.get("softmax_beta", 6.0)
    min_group_size: int = strategy_specific_config.get("min_group_size", 3)
    shrink_lambda: float = strategy_specific_config.get("shrink_lambda", 0.6)

    top_n: int = strategy_specific_config.get("top_n", 3)
    bottom_n: int = strategy_specific_config.get("bottom_n", 3)
    include_leaders_in_trading: bool = strategy_specific_config.get("include_leaders_in_trading", False)
    gross_limit: float = float(strategy_specific_config.get("gross_limit", 1.0))
    zscore_clip: float = float(strategy_specific_config.get("zscore_clip", 3.0))

    # Validate symbols exist in df
    missing_cols = [s for s in symbols if s not in df.columns]
    if missing_cols:
        raise ValueError(f"DataFrame is missing required symbol columns: {missing_cols}")
    if leader_btc not in symbols or leader_eth not in symbols:
        raise ValueError("Leaders must be inside the provided symbols universe.")
    if gross_limit <= 0.0 or gross_limit > 1.0:
        raise ValueError("gross_limit must be in (0, 1].")
    if any([w <= 0 for w in momentum_windows]) or any([l <= 0 for l in leadlag_lags]) or any([r <= 0 for r in reversal_windows]):
        raise ValueError("All window and lag parameters must be positive integers.")
    if forecast_horizon <= 0:
        raise ValueError("forecast_horizon must be positive.")

    # Restrict df to symbols and ensure sorted by time
    df = df.sort_index()
    px = df[symbols].astype(float)
    # Drop columns fully NaN
    px = px.dropna(how='all', axis=1)
    symbols = [s for s in symbols if s in px.columns]
    if len(symbols) < 5:
        # Not enough assets to construct a meaningful cross-sectional portfolio
        return {s: 0.0 for s in df.columns}

    # Compute 1-minute log-returns
    r = np.log(px).diff()
    # Align to drop initial NaNs
    r = r.dropna(how='all')

    # Helper utilities
    def cs_zscore(series: pd.Series, clip: float = None) -> pd.Series:
        s = series.copy()
        mu = s.mean(skipna=True)
        sd = s.std(ddof=0, skipna=True)
        out = (s - mu) / (sd if (sd is not None and sd > 1e-12) else 1.0)
        if clip is not None and np.isfinite(clip):
            out = out.clip(-clip, clip)
        return out

    def ridge_beta(X: np.ndarray, y: np.ndarray, lam: float = 1e-4) -> np.ndarray:
        # Add ridge stabilization
        XtX = X.T @ X
        k = XtX.shape[0]
        return np.linalg.pinv(XtX + lam * np.eye(k)) @ (X.T @ y)

    def safe_var(a: np.ndarray) -> float:
        a = a[np.isfinite(a)]
        if a.size <= 1:
            return 0.0
        return float(np.var(a, ddof=1))

    def safe_cov(x: np.ndarray, y: np.ndarray) -> float:
        m = np.isfinite(x) & np.isfinite(y)
        x, y = x[m], y[m]
        if x.size <= 1:
            return 0.0
        return float(np.cov(x, y, ddof=1)[0,1])

    # Last timestamp t we will trade
    t_idx = r.index[-1]

    # =========================
    # ----- FACTOR: LL --------
    # =========================
    # Build X from leader returns at specified lags, using the last leadlag_lookback window
    lookback_LL = min(leadlag_lookback, len(r) - max(leadlag_lags) - 1)  # ensure enough rows
    if lookback_LL <= 10:
        # insufficient data for LL -> set to zeros
        LL_exp = pd.Series(0.0, index=symbols)
    else:
        window_slice = r.iloc[-(lookback_LL + max(leadlag_lags)):]  # include lag space
        r_btc = window_slice[leader_btc].values
        r_eth = window_slice[leader_eth].values

        # Build design matrix for the aligned window
        rows = []
        for L in leadlag_lags:
            # Align so that for time t, predictor uses leaders at t-L
            rows.append(r_btc[max(leadlag_lags)-L : -L])
            rows.append(r_eth[max(leadlag_lags)-L : -L])
        X = np.vstack(rows).T  # shape (T, 2*len(lags))

        # For each asset, estimate betas and compute exposure at current time using latest lagged leaders
        LL_vals = {}
        # Latest lagged leaders vector x_t
        x_t = []
        for L in leadlag_lags:
            x_t.append(r[leader_btc].iloc[-L])
            x_t.append(r[leader_eth].iloc[-L])
        x_t = np.array(x_t)

        for s in symbols:
            y = window_slice[s].values[max(leadlag_lags):]  # align with X
            if np.isfinite(y).sum() < 10:
                LL_vals[s] = 0.0
                continue
            mask = np.all(np.isfinite(X), axis=1) & np.isfinite(y)
            X_m = X[mask]
            y_m = y[mask]
            if len(y_m) <= 10:
                LL_vals[s] = 0.0
                continue
            beta = ridge_beta(X_m, y_m, lam=1e-3)
            LL_vals[s] = float(np.dot(beta, x_t))
        LL_exp = pd.Series(LL_vals)
        LL_exp = cs_zscore(LL_exp, zscore_clip)

    # =========================
    # ----- FACTOR: MOM -------
    # =========================
    # Residual momentum neutralized to contemporaneous BTC/ETH using rolling betas over beta_lookback
    lookback_beta = min(beta_lookback, len(r) - 2)
    if lookback_beta <= 10:
        MOM_exp = pd.Series(0.0, index=symbols)
    else:
        # Estimate betas over last lookback_beta window and form residuals over last max(momentum_windows)
        r_win = r.iloc[-(lookback_beta + max(momentum_windows) + forecast_horizon + 5):].copy()
        r_btc = r_win[leader_btc].values
        r_eth = r_win[leader_eth].values
        # Beta window is the last 'lookback_beta' rows prior to current t
        r_beta_slice = r_win.iloc[:lookback_beta]
        X_beta = np.column_stack([r_beta_slice[leader_btc].values, r_beta_slice[leader_eth].values])
        # Prepare residuals per asset using estimated beta (constant-free)
        res_df = pd.DataFrame(index=r_win.index, columns=symbols, dtype=float)
        for s in symbols:
            yb = r_beta_slice[s].values
            m = np.all(np.isfinite(X_beta), axis=1) & np.isfinite(yb)
            if m.sum() <= 10:
                res_df[s] = np.nan
                continue
            b = ridge_beta(X_beta[m], yb[m], lam=1e-3)  # betas to BTC/ETH
            # residuals over the entire r_win using these betas
            X_full = np.column_stack([r_win[leader_btc].values, r_win[leader_eth].values])
            y_full = r_win[s].values
            res = y_full - X_full @ b
            res_df[s] = res

        # Momentum exposure at t: average of rolling sums over windows of residuals (ending at t)
        MOM_vals = {}
        for s in symbols:
            series = res_df[s].astype(float)
            if series.isna().all():
                MOM_vals[s] = 0.0
                continue
            mom_components = []
            for w in momentum_windows:
                if len(series.dropna()) < w + 1:
                    mom_components.append(0.0)
                else:
                    mom_components.append(series.iloc[-w:].sum())
            MOM_vals[s] = float(np.nanmean(mom_components))
        MOM_exp = pd.Series(MOM_vals)
        MOM_exp = cs_zscore(MOM_exp, zscore_clip)

    # =========================
    # ----- FACTOR: REV -------
    # =========================
    REV_vals = {}
    for s in symbols:
        comps = []
        for w in reversal_windows:
            if len(r[s].dropna()) < w + 1:
                comps.append(0.0)
            else:
                comps.append(r[s].iloc[-w:].sum())
        REV_vals[s] = -float(np.nanmean(comps))
    REV_exp = pd.Series(REV_vals)
    REV_exp = cs_zscore(REV_exp, zscore_clip)

    # =========================
    # ---- REGIME GATING ------
    # =========================
    # Variance Ratio (VR) on BTC with horizon vr_q:
    # VR = Var(sum_q r) / [q * Var(r)] - 1
    # Use rolling samples of q-sum across a broader window for variance estimation
    btc_r = r[leader_btc].astype(float)
    W = max(vr_q * 20, vr_q + 10)
    if len(btc_r) < W + vr_q + 1:
        VR = 0.0
    else:
        x = btc_r.iloc[-(W + vr_q - 1):].values
        # build overlapping q-sums
        qsums = np.convolve(x, np.ones(vr_q), mode='valid')
        var_qsum = safe_var(qsums)
        var_r = safe_var(x)
        denom = vr_q * var_r if vr_q * var_r > 1e-12 else 1e-12
        VR = var_qsum / denom - 1.0
    G_pos = max(0.0, VR) * vr_gate_strength
    G_neg = max(0.0, -VR) * vr_gate_strength

    MOM_g = MOM_exp * G_pos
    REV_g = REV_exp * G_neg
    # LL is not gated
    LL_g = LL_exp.copy()

    # =========================
    # ---- COST GATE (Roll) ---
    # =========================
    # S_hat = 2 * sqrt(max(0, -Cov(r_t, r_{t-1}))) estimated over last roll_cov_window
    S_hat = {}
    if len(r) < roll_cov_window + 2:
        # insufficient for robust estimate
        for s in symbols:
            S_hat[s] = 0.0
    else:
        r_roll = r.iloc[-(roll_cov_window + 1):]
        for s in symbols:
            x = r_roll[s].values
            cov_lag1 = safe_cov(x[1:], x[:-1])
            proxy = 2.0 * np.sqrt(max(0.0, -cov_lag1))
            S_hat[s] = float(proxy)
    S_hat = pd.Series(S_hat)

    # =========================
    # --- SUBGROUP IC WEIGHTS -
    # =========================
    # Build factor exposure histories to compute ICs w.r.t forward returns over time.
    # To keep runtime manageable, approximate by recomputing factor exposures historically
    # over a moderately recent window that can support forecast horizon.
    hist_len = int(max(leadlag_lookback, beta_lookback, roll_cov_window, max(momentum_windows) + forecast_horizon + 10))
    hist_len = min(hist_len, len(r) - forecast_horizon - max(leadlag_lags) - 5) if len(r) > 100 else max(50, hist_len)
    if hist_len <= 50:
        # Fallback: if data too short, use equal weights for factors
        group_factor_weights = {"pow": np.array([1/3, 1/3, 1/3]),
                                "platform": np.array([1/3, 1/3, 1/3]),
                                "meme": np.array([1/3, 1/3, 1/3]),
                                "other": np.array([1/3, 1/3, 1/3])}
    else:
        start_idx = len(r) - hist_len - forecast_horizon
        r_hist = r.iloc[start_idx:]
        # Precompute forward returns
        fwd_r = r_hist.shift(-forecast_horizon)  # forward returns at t -> realized at t+H
        # Build simple historical approximations of factor exposures by reusing contemporary rules with rolling slices
        # MOM/REV are cheaper; LL requires lagged leaders.

        def build_LL_series(r_frame: pd.DataFrame) -> pd.DataFrame:
            # Compute LL exposure time series for each t where lag structure available; use rolling betas over leadlag_lookback (fixed across the small history to reduce compute).
            LL_series = pd.DataFrame(index=r_frame.index, columns=symbols, dtype=float)
            Lmax = max(leadlag_lags)
            if len(r_frame) <= leadlag_lookback + Lmax + 5:
                return LL_series
            # Estimate betas over first usable block (last leadlag_lookback rows before the evaluation window end); to keep efficient, re-estimate once using last 'leadlag_lookback' prior to each timestamp blockwise.
            for tpos in range(leadlag_lookback + Lmax, len(r_frame)):
                window_slice = r_frame.iloc[tpos - leadlag_lookback - Lmax : tpos]
                rb = window_slice[leader_btc].values
                re = window_slice[leader_eth].values
                rows = []
                for L in leadlag_lags:
                    rows.append(rb[Lmax - L : -L])
                    rows.append(re[Lmax - L : -L])
                X = np.vstack(rows).T
                x_t = []
                for L in leadlag_lags:
                    x_t.append(r_frame[leader_btc].iloc[tpos - L])
                    x_t.append(r_frame[leader_eth].iloc[tpos - L])
                x_t = np.array(x_t)
                for s in symbols:
                    y = window_slice[s].values[Lmax:]
                    mask = np.all(np.isfinite(X), axis=1) & np.isfinite(y)
                    if mask.sum() <= 10:
                        LL_series.iat[tpos, LL_series.columns.get_loc(s)] = np.nan
                        continue
                    beta_hat = ridge_beta(X[mask], y[mask], lam=1e-3)
                    LL_series.iat[tpos, LL_series.columns.get_loc(s)] = float(np.dot(beta_hat, x_t))
            # Cross-sectional z-score each timestamp
            LL_series = LL_series.apply(lambda row: cs_zscore(row, zscore_clip), axis=1)
            return LL_series

        def build_MOM_series(r_frame: pd.DataFrame) -> pd.DataFrame:
            MOM_series = pd.DataFrame(index=r_frame.index, columns=symbols, dtype=float)
            if len(r_frame) <= beta_lookback + max(momentum_windows) + 5:
                return MOM_series
            # Estimate neutralization betas on a rolling basis sparsely (use trailing beta_lookback ending at each t)
            for tpos in range(beta_lookback + max(momentum_windows), len(r_frame)):
                r_beta_slice = r_frame.iloc[tpos - beta_lookback - 0 : tpos]
                X_beta = np.column_stack([r_beta_slice[leader_btc].values, r_beta_slice[leader_eth].values])
                # residuals over last max window
                for s in symbols:
                    yb = r_beta_slice[s].values
                    mask = np.all(np.isfinite(X_beta), axis=1) & np.isfinite(yb)
                    if mask.sum() <= 10:
                        MOM_series.iat[tpos, MOM_series.columns.get_loc(s)] = np.nan
                        continue
                    b = ridge_beta(X_beta[mask], yb[mask], lam=1e-3)
                    X_curr = np.column_stack([r_frame[leader_btc].values, r_frame[leader_eth].values])
                    y_curr = r_frame[s].values
                    res = y_curr - X_curr @ b
                    # momentum comp ending at tpos
                    comps = []
                    for w in momentum_windows:
                        if tpos - w + 1 < 0:
                            comps.append(np.nan)
                        else:
                            window = res[tpos - w + 1 : tpos + 1]
                            comps.append(np.nansum(window))
                    MOM_series.iat[tpos, MOM_series.columns.get_loc(s)] = float(np.nanmean(comps))
            MOM_series = MOM_series.apply(lambda row: cs_zscore(row, zscore_clip), axis=1)
            return MOM_series

        def build_REV_series(r_frame: pd.DataFrame) -> pd.DataFrame:
            REV_series = pd.DataFrame(index=r_frame.index, columns=symbols, dtype=float)
            for tpos in range(max(reversal_windows), len(r_frame)):
                for s in symbols:
                    comps = []
                    for w in reversal_windows:
                        window = r_frame[s].values[tpos - w + 1 : tpos + 1]
                        if np.isfinite(window).sum() < w:
                            comps.append(np.nan)
                        else:
                            comps.append(window.sum())
                    REV_series.iat[tpos, REV_series.columns.get_loc(s)] = -float(np.nanmean(comps))
            REV_series = REV_series.apply(lambda row: cs_zscore(row, zscore_clip), axis=1)
            return REV_series

        # Build factor histories
        LL_hist = build_LL_series(r_hist.copy())
        MOM_hist = build_MOM_series(r_hist.copy())
        REV_hist = build_REV_series(r_hist.copy())

        # Apply regime gating historically (approximate using BTC VR computed over rolling window as above)
        # Compute VR time series (using same approach)
        VR_series = pd.Series(index=r_hist.index, dtype=float)
        xbtc = r_hist[leader_btc].astype(float).values
        for tpos in range(vr_q * 25, len(r_hist) - 1):
            seg = xbtc[: tpos + 1]
            if len(seg) < vr_q + 10:
                VR_series.iat[tpos] = np.nan
                continue
            qsums = np.convolve(seg[-max(vr_q * 20, vr_q + 10):], np.ones(vr_q), mode='valid')
            var_qsum = safe_var(qsums)
            var_rv = safe_var(seg[-max(vr_q * 20, vr_q + 10):])
            denom = vr_q * var_rv if vr_q * var_rv > 1e-12 else 1e-12
            VR_series.iat[tpos] = var_qsum / denom - 1.0
        Gpos_series = VR_series.clip(lower=0.0) * vr_gate_strength
        Gneg_series = (-VR_series).clip(lower=0.0) * vr_gate_strength

        MOM_hist_g = (MOM_hist.T * Gpos_series).T
        REV_hist_g = (REV_hist.T * Gneg_series).T
        LL_hist_g = LL_hist  # unchanged

        # Compute subgroup-specific IC time series (Spearman rank correlation at each timestamp)
        def rank_ic_at_t(factor_row: pd.Series, fwd_row: pd.Series, members: List[str]) -> float:
            xs = factor_row[members]
            ys = fwd_row[members]
            xs = xs.replace([np.inf, -np.inf], np.nan).dropna()
            ys = ys.replace([np.inf, -np.inf], np.nan)
            common = xs.index.intersection(ys.dropna().index)
            if len(common) < 2:
                return np.nan
            xr = xs.loc[common].rank()
            yr = ys.loc[common].rank()
            xv = xr.values
            yv = yr.values
            if xv.std(ddof=0) < 1e-12 or yv.std(ddof=0) < 1e-12:
                return np.nan
            return float(np.corrcoef(xv, yv)[0,1])

        groups = {
            "pow": [s for s in group_pow if s in symbols],
            "platform": [s for s in group_platform if s in symbols],
            "meme": [s for s in group_meme if s in symbols],
            "other": [s for s in group_other if s in symbols],
        }

        # Build forward returns aligned to factor timestamps
        fwd = fwd_r.loc[LL_hist_g.index]  # align index

        factor_mats = {
            "LL": LL_hist_g,
            "MOM": MOM_hist_g,
            "REV": REV_hist_g
        }

        # Global IC (across all symbols) time series per factor
        global_ic_ts = {k: [] for k in factor_mats.keys()}
        idxs = []
        for idx in factor_mats["LL"].index:
            if idx not in fwd.index:
                continue
            idxs.append(idx)
            for k, mat in factor_mats.items():
                global_ic_ts[k].append(rank_ic_at_t(mat.loc[idx], fwd.loc[idx], [s for s in symbols if s in mat.columns]))
        global_ic_ts = {k: pd.Series(v, index=idxs, dtype=float) for k, v in global_ic_ts.items()}
        # Smooth globals
        global_ic_sm = {k: v.ewm(halflife=ic_halflife, adjust=False, min_periods=5).mean() for k, v in global_ic_ts.items()}

        # Group ICs and shrink toward global
        group_factor_weights = {}
        for gname, members in groups.items():
            # If subgroup too small, treat as empty (we'll still compute weights via shrinkage)
            group_ic_ts = {k: [] for k in factor_mats.keys()}
            valid_len = 0
            idxs_g = []
            for idx in idxs:
                if len(members) >= 2:
                    ics = {k: rank_ic_at_t(factor_mats[k].loc[idx], fwd.loc[idx], members) for k in factor_mats.keys()}
                    valid_len += int(all([np.isfinite(v) for v in ics.values()]))
                else:
                    ics = {k: np.nan for k in factor_mats.keys()}
                for k in factor_mats.keys():
                    group_ic_ts[k].append(ics[k])
                idxs_g.append(idx)
            group_ic_ts = {k: pd.Series(v, index=idxs_g, dtype=float) for k, v in group_ic_ts.items()}
            group_ic_sm = {k: v.ewm(halflife=ic_halflife, adjust=False, min_periods=5).mean() for k, v in group_ic_ts.items()}

            # Shrink each factor's IC at the most recent timestamp
            current_weights_vec = []
            for k in ["LL", "MOM", "REV"]:
                ic_group = group_ic_sm[k].iloc[-1] if len(group_ic_sm[k]) else np.nan
                ic_global = global_ic_sm[k].iloc[-1] if len(global_ic_sm[k]) else np.nan
                if not np.isfinite(ic_group):
                    ic_group = np.nanmean(group_ic_sm[k].values) if len(group_ic_sm[k]) else np.nan
                if not np.isfinite(ic_global):
                    ic_global = 0.0
                # min_group_size gating
                if len(members) < max(2, min_group_size):
                    ic_eff = ic_global
                else:
                    ic_eff = shrink_lambda * ic_group + (1.0 - shrink_lambda) * ic_global
                if not np.isfinite(ic_eff):
                    ic_eff = 0.0
                current_weights_vec.append(ic_eff)

            # Softmax over non-negative scores; temperature beta
            v = np.array(current_weights_vec, dtype=float)
            # Shift so that negatives don't dominate; we only want non-negative weights
            v = np.maximum(v, 0.0)
            if np.all(v <= 0):
                w = np.array([1/3, 1/3, 1/3], dtype=float)
            else:
                x = softmax_beta * v
                x = x - np.max(x)
                ex = np.exp(x)
                w = ex / (ex.sum() if ex.sum() > 1e-12 else 1.0)
            group_factor_weights[gname] = w

    # =========================
    # ---- SCORE CONSTRUCTION -
    # =========================
    # map symbol -> group name
    sym2group = {}
    for s in symbols:
        if s in group_pow:
            sym2group[s] = "pow"
        elif s in group_platform:
            sym2group[s] = "platform"
        elif s in group_meme:
            sym2group[s] = "meme"
        else:
            sym2group[s] = "other"

    # Build scores with group weights
    scores = {}
    for s in symbols:
        gname = sym2group.get(s, "other")
        wg = group_factor_weights.get(gname, np.array([1/3, 1/3, 1/3]))
        # wg order: [LL, MOM, REV]
        sc = float(wg[0] * LL_g.get(s, 0.0) + wg[1] * MOM_g.get(s, 0.0) + wg[2] * REV_g.get(s, 0.0))
        scores[s] = sc
    scores = pd.Series(scores)

    # Apply cost gate
    # If |score_i| < cost_kappa * S_hat_i then set score_i = 0
    S_use = S_hat.reindex(index=symbols).fillna(0.0)
    gated_scores = scores.copy()
    mask_small = gated_scores.abs() < (cost_kappa * S_use.replace(0.0, np.nan).fillna(0.0))
    gated_scores[mask_small] = 0.0

    # Optionally exclude leaders from trading
    tradable = symbols.copy()
    if not include_leaders_in_trading:
        tradable = [s for s in tradable if s not in [leader_btc, leader_eth]]

    trad_scores = gated_scores.reindex(tradable).replace([np.inf, -np.inf], np.nan).fillna(0.0)

    # If all zero, return zero weights
    if trad_scores.abs().sum() <= 1e-12:
        return {s: 0.0 for s in df.columns}

    # =========================
    # --- PORTFOLIO BUILD -----
    # =========================
    # Rank, go long top_n, short bottom_n; dollar-neutral target 0.5 gross per side
    # Use absolute score-proportional weights within each side
    # Select longs and shorts
    # Ensure we have at least 1 on each side; if not, allocate only where available, still cap gross
    trad_scores_sorted = trad_scores.sort_values()
    shorts = trad_scores_sorted.head(bottom_n)
    longs = trad_scores_sorted.tail(top_n)

    # Remove zeros after gating to avoid tiny allocations
    longs = longs[longs > 0]
    shorts = shorts[shorts < 0]

    weights = {s: 0.0 for s in df.columns}

    def allocate(side_series: pd.Series, target_gross: float) -> Dict[str, float]:
        w = {}
        if side_series.empty:
            return w
        sizes = side_series.abs()
        total = sizes.sum()
        if total <= 1e-12:
            return w
        for sym, val in side_series.items():
            w[sym] = float((val / total) * target_gross)  # sign preserved automatically (val carries sign)
        return w

    w_long = allocate(longs, 0.5)
    w_short = allocate(shorts, 0.5)

    # Combine and scale to respect gross_limit and hard cap of 1.0
    combined = {}
    for s in set(list(w_long.keys()) + list(w_short.keys())):
        combined[s] = w_long.get(s, 0.0) + w_short.get(s, 0.0)

    gross = sum(abs(v) for v in combined.values())
    if gross > 0:
        scale = min(gross_limit, 1.0) / gross
    else:
        scale = 0.0
    for s in combined:
        combined[s] *= scale

    # Final safety cap if numerical issues push above 1.0
    gross_final = sum(abs(v) for v in combined.values())
    if gross_final > 1.0:
        for s in combined:
            combined[s] *= (1.0 / gross_final)

    # Fill weights dictionary for all symbols (0 if not allocated)
    for s in combined:
        weights[s] = float(combined[s])

    # Return final weights
    return weights

